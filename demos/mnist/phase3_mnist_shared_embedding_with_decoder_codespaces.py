# -*- coding: utf-8 -*-
"""
MNIST è·¨æ¨¡æ€è¯­ä¹‰å¯¹é½ï¼ˆå…±äº«å‘é‡ç©ºé—´ï¼‰ + è§£ç å™¨
=============================================
è·¯çº¿2ï¼šå¯¹æ¯”å­¦ä¹ å¯¹é½ embedding + æ¯ä¸ªé€šé“è‡ªå¸¦ decoder

æ¶æ„:
  - Image Encoder (CNN): å›¾åƒ -> embedding
  - Image Decoder (CNN): embedding -> å›¾åƒ
  - Text Encoder: digit id (0-9) -> embedding (nn.Embedding lookup)
  - å¯¹é½: å›¾åƒ embedding ä¸æ–‡æœ¬ embedding åœ¨å½’ä¸€åŒ–ç©ºé—´ä¸­å¯¹é½

ä¸‰ä¸ªæŸå¤±å‡½æ•°è”åˆè®­ç»ƒ:
  (1) å¯¹é½æŸå¤±: CrossEntropy(image_emb @ text_emb.T, label) â€” è®©åŒç±»å¯¹é½
  (2) å›¾åƒé‡å»ºæŸå¤±: MSE(Decoder(Encoder(image)), image) â€” è®© encoder ä¿ç•™é‡å»ºä¿¡æ¯
  (3) æ–‡æœ¬è§£ç æŸå¤±: MSE(Decoder(TextEmbed(label)), image) â€” â˜… å…³é”®ï¼
      è®© decoder å­¦ä¼šä»æ–‡æœ¬ embedding è§£ç å›¾åƒï¼Œå¦åˆ™è·¨æ¨¡æ€ç”Ÿæˆä¸å¯ç”¨

è®­ç»ƒå®Œæˆåå¯ä»¥:
  - Image -> Text: å›¾åƒç¼–ç åä¸10ä¸ªæ–‡æœ¬embeddingæ¯”è¾ƒï¼Œå–æœ€ç›¸ä¼¼çš„ = çœ‹å›¾è¯†æ•°å­—
  - Text -> Image: æ–‡æœ¬embeddingç›´æ¥é€å…¥decoder = ä»æ•°å­—ç”Ÿæˆå›¾åƒï¼ˆæ¡ä»¶ç”Ÿæˆï¼‰
  - Image -> Image: å›¾åƒç¼–ç å†è§£ç  = è‡ªç¼–ç å™¨é‡å»º

è¯„ä¼°æ–¹å¼:
  - åˆ†ç±»å‡†ç¡®ç‡: Image -> Text çš„è¯†åˆ«å‡†ç¡®ç‡
  - é‡å»º MSE: å›¾åƒè‡ªç¼–ç é‡å»ºè¯¯å·®
  - å¤–éƒ¨åˆ†ç±»å™¨å‡†ç¡®ç‡: ç”Ÿæˆçš„å›¾åƒèƒ½å¦è¢«ç‹¬ç«‹åˆ†ç±»å™¨è¯†åˆ« (å®¢è§‚æŒ‡æ ‡ï¼Œé¿å…è‡ªè¯„åå·®)
  - å¤–éƒ¨åˆ†ç±»å™¨ç½®ä¿¡åº¦: ç‹¬ç«‹åˆ†ç±»å™¨å¯¹æ­£ç¡®ç±»åˆ«çš„ softmax æ¦‚ç‡ (è¶Šé«˜è¶Šå¥½)

è¿è¡Œæ–¹å¼:
    python mnist_shared_embedding_with_decoder.py
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import matplotlib
if os.environ.get('DISPLAY', '') == '':
    matplotlib.use('Agg')
import matplotlib.pyplot as plt

# -----------------------------
# Codespaces helper: save plots
# -----------------------------
# Codespaces is usually headless, so figures won't pop up. We save them to ./outputs/.
def save_plot(filename: str, dpi: int = 150):
    os.makedirs("outputs", exist_ok=True)
    out_path = os.path.join("outputs", filename)
    plt.savefig(out_path, dpi=dpi, bbox_inches="tight")
    print(f"[Plot saved] {out_path}")
    plt.close()

# =====================================================================
# 1) è¶…å‚æ•°
# =====================================================================
EMB_DIM    = 128       # embedding ç»´åº¦ (è¶Šå¤§ä¿¡æ¯ç“¶é¢ˆè¶Šå®½ï¼Œé‡å»ºè¶Šå¥½)
EPOCHS     = 20        # è®­ç»ƒè½®æ¬¡ (é‡å»ºæ¯”åˆ†ç±»éœ€è¦æ›´å¤šè½®)
LR         = 1e-3
TEMP       = 0.07      # ä½™å¼¦ç›¸ä¼¼åº¦çš„æ¸©åº¦å‚æ•°
LAMBDA_RECON      = 1.0   # å›¾åƒé‡å»ºæŸå¤±æƒé‡
LAMBDA_TEXT_RECON  = 1.0   # æ–‡æœ¬è§£ç æŸå¤±æƒé‡
MODEL_PATH = "mnist_shared_with_decoder.pth"


# =====================================================================
# 2) æ•°æ®åŠ è½½
# =====================================================================
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # åƒç´ å½’ä¸€åŒ–åˆ° [-1, 1]
])

trainset = torchvision.datasets.MNIST(root="/workspaces/ai101-starter/data_local/data", train=True, download=True, transform=transform)
testset  = torchvision.datasets.MNIST(root="/workspaces/ai101-starter/data_local/data", train=False, download=True, transform=transform)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,  shuffle=True)
testloader  = torch.utils.data.DataLoader(testset,  batch_size=256, shuffle=False)


# =====================================================================
# 3) æ¨¡å‹å®šä¹‰: CNN ç¼–ç å™¨ + CNN è§£ç å™¨ + æ–‡æœ¬ Embedding
# =====================================================================
class ImageEncoder(nn.Module):
    """
    CNN ç¼–ç å™¨: (B, 1, 28, 28) -> (B, emb_dim)
    é€šè¿‡ stride=2 çš„å·ç§¯é€æ­¥ä¸‹é‡‡æ ·: 28 -> 14 -> 7 -> 4
    BatchNorm åŠ é€Ÿæ”¶æ•›ï¼ŒReLU å¼•å…¥éçº¿æ€§
    """
    def __init__(self, emb_dim: int = 128):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1,  32, 3, stride=2, padding=1),  # (B,32,14,14)
            nn.BatchNorm2d(32), nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # (B,64,7,7)
            nn.BatchNorm2d(64), nn.ReLU(),
            nn.Conv2d(64, 64, 3, stride=2, padding=1),  # (B,64,4,4)
            nn.BatchNorm2d(64), nn.ReLU(),
        )
        self.fc = nn.Linear(64 * 4 * 4, emb_dim)

    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)  # (B, 1024)
        return self.fc(x)           # (B, emb_dim)


class ImageDecoder(nn.Module):
    """
    CNN è§£ç å™¨: (B, emb_dim) -> (B, 1, 28, 28)
    é€šè¿‡ ConvTranspose2d é€æ­¥ä¸Šé‡‡æ ·: 4 -> 7 -> 14 -> 28
    æœ€åç”¨ Tanh è¾“å‡º [-1, 1]ï¼Œä¸è¾“å…¥æ•°æ®èŒƒå›´å¯¹é½
    """
    def __init__(self, emb_dim: int = 128):
        super().__init__()
        self.fc = nn.Linear(emb_dim, 64 * 4 * 4)
        self.deconv = nn.Sequential(
            nn.ConvTranspose2d(64, 64, 3, stride=2, padding=1, output_padding=0),  # (B,64,7,7)
            nn.BatchNorm2d(64), nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),  # (B,32,14,14)
            nn.BatchNorm2d(32), nn.ReLU(),
            nn.ConvTranspose2d(32,  1, 3, stride=2, padding=1, output_padding=1),  # (B,1,28,28)
            nn.Tanh(),
        )

    def forward(self, z):
        z = self.fc(z)               # (B, 1024)
        z = z.view(-1, 64, 4, 4)     # (B, 64, 4, 4)
        return self.deconv(z)         # (B, 1, 28, 28)


class SharedSpaceWithDecoder(nn.Module):
    """
    å®Œæ•´æ¨¡å‹: å›¾åƒç¼–ç å™¨ + å›¾åƒè§£ç å™¨ + æ–‡æœ¬Embedding
    - å¯¹é½åœ¨ L2 å½’ä¸€åŒ–åçš„ç©ºé—´å®Œæˆ (ä½™å¼¦ç›¸ä¼¼åº¦)
    - é‡å»ºåœ¨ raw embedding ç©ºé—´å®Œæˆ (ä¿ç•™å®Œæ•´ä¿¡æ¯)
    """
    def __init__(self, emb_dim: int = 128, num_text_tokens: int = 10, temperature: float = 0.07):
        super().__init__()
        self.image_encoder = ImageEncoder(emb_dim=emb_dim)
        self.image_decoder = ImageDecoder(emb_dim=emb_dim)
        self.text_embed = nn.Embedding(num_text_tokens, emb_dim)
        self.temperature = temperature

    @staticmethod
    def l2_normalize(x: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
        return x / (x.norm(dim=-1, keepdim=True) + eps)

    def encode_image(self, images: torch.Tensor) -> torch.Tensor:
        """å›¾åƒ -> L2 å½’ä¸€åŒ–åçš„ embedding (ç”¨äºå¯¹é½/åˆ†ç±»)"""
        return self.l2_normalize(self.image_encoder(images))

    def encode_text_all(self) -> torch.Tensor:
        """è·å–å…¨éƒ¨ 10 ä¸ªæ–‡æœ¬ embedding (L2 å½’ä¸€åŒ–)"""
        return self.l2_normalize(self.text_embed.weight)

    def logits_image_to_text(self, images: torch.Tensor) -> torch.Tensor:
        """Image -> Text: è¿”å› (B, 10) çš„ç›¸ä¼¼åº¦ logits"""
        img_z = self.encode_image(images)     # (B, D)
        txt_z = self.encode_text_all()        # (10, D)
        return (img_z @ txt_z.t()) / self.temperature

    def reconstruct(self, images: torch.Tensor):
        """Image -> Encoder -> Decoder -> Image (è‡ªç¼–ç é‡å»º)"""
        z_raw = self.image_encoder(images)      # (B, D) æœªå½’ä¸€åŒ–
        return self.image_decoder(z_raw)        # (B, 1, 28, 28)

    def decode_from_text(self, digit_id: torch.Tensor):
        """Text -> Image: æ–‡æœ¬ embedding ç›´æ¥é€å…¥ decoder ç”Ÿæˆå›¾åƒ"""
        z = self.text_embed(digit_id)  # (B, D)
        return self.image_decoder(z)   # (B, 1, 28, 28)


# =====================================================================
# 4) ç‹¬ç«‹åˆ†ç±»å™¨ (ç”¨äºå®¢è§‚è¯„ä¼°ç”Ÿæˆå›¾åƒè´¨é‡)
# =====================================================================
# ä¸ºä»€ä¹ˆéœ€è¦ç‹¬ç«‹åˆ†ç±»å™¨?
# å¦‚æœç”¨åŒä¸€ä¸ªæ¨¡å‹çš„ logits_image_to_text æ¥è¯„åˆ¤è‡ªå·±ç”Ÿæˆçš„å›¾åƒï¼Œ
# å­˜åœ¨"è‡ªè¯„åå·®"â€”â€”æ¨¡å‹å¯èƒ½å¯¹è‡ªå·±çš„è¾“å‡ºç‰¹åˆ«"å®½å®¹"ã€‚
# ç‹¬ç«‹åˆ†ç±»å™¨æ˜¯å®Œå…¨ç‹¬ç«‹è®­ç»ƒçš„ï¼Œèƒ½æ›´å®¢è§‚åœ°è¯„åˆ¤ç”Ÿæˆå›¾åƒæ˜¯å¦åƒå¯¹åº”æ•°å­—ã€‚

class IndependentClassifier(nn.Module):
    """ç®€å•çš„ MLP åˆ†ç±»å™¨ï¼Œç‹¬ç«‹äºä¸»æ¨¡å‹"""
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Flatten(),
            nn.Linear(784, 256), nn.ReLU(),
            nn.Linear(256, 128), nn.ReLU(),
            nn.Linear(128, 10),
        )
    def forward(self, x):
        return self.net(x)


# =====================================================================
# 5) è®­ç»ƒ / è½½å…¥
# =====================================================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"è®¾å¤‡: {device}")

model = SharedSpaceWithDecoder(emb_dim=EMB_DIM, temperature=TEMP).to(device)

criterion_align = nn.CrossEntropyLoss()
criterion_recon = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=LR)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)


def evaluate_accuracy(model, dataloader) -> float:
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            pred = model.logits_image_to_text(images).argmax(1)
            correct += (pred == labels).sum().item()
            total += labels.numel()
    return correct / max(total, 1)


if os.path.exists(MODEL_PATH):
    print(f"ğŸ”„ æ£€æµ‹åˆ°å·²æœ‰æ¨¡å‹ï¼Œæ­£åœ¨åŠ è½½ {MODEL_PATH} ...")
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
else:
    print("â³ æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹ï¼Œå¼€å§‹è®­ç»ƒï¼ˆå¯¹é½ + å›¾åƒé‡å»º + æ–‡æœ¬è§£ç ï¼‰...")

    for epoch in range(EPOCHS):
        model.train()
        running_loss = running_align = running_recon = running_text = 0.0

        for images, labels in trainloader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()

            # (1) å¯¹é½æŸå¤±: å›¾åƒ embedding ä¸å¯¹åº”æ–‡æœ¬ embedding å¯¹é½
            logits = model.logits_image_to_text(images)
            loss_align = criterion_align(logits, labels)

            # (2) å›¾åƒé‡å»ºæŸå¤±: Image -> Encoder -> Decoder -> Image
            recon = model.reconstruct(images)
            loss_recon = criterion_recon(recon, images)

            # (3) â˜… æ–‡æœ¬è§£ç æŸå¤±: TextEmbed(label) -> Decoder -> Image
            #     è¿™æ˜¯è®©è·¨æ¨¡æ€ç”Ÿæˆå¯ç”¨çš„å…³é”®ï¼
            #     å¦‚æœä¸åŠ è¿™ä¸ªæŸå¤±ï¼Œdecoder ä»æœªè§è¿‡æ–‡æœ¬ embeddingï¼Œ
            #     æ¨ç†æ—¶ Text->Image å°±ä¼šç”Ÿæˆåƒåœ¾ã€‚
            text_recon = model.decode_from_text(labels)
            loss_text_recon = criterion_recon(text_recon, images)

            loss = loss_align + LAMBDA_RECON * loss_recon + LAMBDA_TEXT_RECON * loss_text_recon
            loss.backward()
            optimizer.step()

            running_loss  += loss.item()
            running_align += loss_align.item()
            running_recon += loss_recon.item()
            running_text  += loss_text_recon.item()

        scheduler.step()
        n = len(trainloader)
        cur_lr = scheduler.get_last_lr()[0]
        test_acc = evaluate_accuracy(model, testloader)
        print(f"  Epoch {epoch+1:2d}/{EPOCHS} | lr={cur_lr:.6f} | loss={running_loss/n:.4f} "
              f"(align={running_align/n:.4f} img_recon={running_recon/n:.4f} txt_recon={running_text/n:.4f}) "
              f"| test_acc={test_acc*100:.2f}%")

    torch.save(model.state_dict(), MODEL_PATH)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³ {MODEL_PATH}")


# =====================================================================
# 6) è®­ç»ƒç‹¬ç«‹åˆ†ç±»å™¨ (ä½œä¸ºå¤–éƒ¨è£åˆ¤)
# =====================================================================
print("\nğŸ“Š è®­ç»ƒç‹¬ç«‹åˆ†ç±»å™¨ï¼ˆç”¨äºå®¢è§‚è¯„ä¼°ç”Ÿæˆè´¨é‡ï¼‰...")
ext_clf = IndependentClassifier().to(device)
ext_opt = optim.Adam(ext_clf.parameters(), lr=1e-3)
ext_ce = nn.CrossEntropyLoss()
for ep in range(5):
    ext_clf.train()
    for imgs, lbls in trainloader:
        imgs, lbls = imgs.to(device), lbls.to(device)
        ext_opt.zero_grad()
        loss = ext_ce(ext_clf(imgs), lbls)
        loss.backward(); ext_opt.step()
ext_clf.eval()
ext_correct = ext_total = 0
with torch.no_grad():
    for imgs, lbls in testloader:
        imgs, lbls = imgs.to(device), lbls.to(device)
        ext_correct += (ext_clf(imgs).argmax(1) == lbls).sum().item()
        ext_total += lbls.numel()
print(f"   ç‹¬ç«‹åˆ†ç±»å™¨å‡†ç¡®ç‡: {ext_correct/ext_total*100:.2f}%\n")


# =====================================================================
# 7) ç»¼åˆè¯„ä¼°
# =====================================================================
model.eval()

# (a) Image -> Text åˆ†ç±»å‡†ç¡®ç‡
test_acc = evaluate_accuracy(model, testloader)

# (b) Image é‡å»º MSE
sample_images = torch.stack([testset[i][0] for i in range(10)]).to(device)
with torch.no_grad():
    recon_images = model.reconstruct(sample_images)
    recon_mse = nn.functional.mse_loss(recon_images, sample_images).item()

# (c) Text -> Image ç”Ÿæˆè´¨é‡ (ç‹¬ç«‹åˆ†ç±»å™¨è¯„ä¼°)
#     å¯¹æ¯ä¸ªæ•°å­— d (0-9):
#       1. text_embed(d) -> decoder -> ç”Ÿæˆå›¾åƒ
#       2. ç”Ÿæˆå›¾åƒ -> ç‹¬ç«‹åˆ†ç±»å™¨ -> é¢„æµ‹ & ç½®ä¿¡åº¦
ext_gen_correct = 0
ext_gen_conf_sum = 0.0
gen_results = {}  # å­˜å‚¨æ¯ä¸ªæ•°å­—çš„è¯¦ç»†ç»“æœ

with torch.no_grad():
    for d in range(10):
        digit_id = torch.tensor([d], device=device)
        gen_img = model.decode_from_text(digit_id)

        # ç‹¬ç«‹åˆ†ç±»å™¨åˆ¤æ–­
        ext_logits = ext_clf(gen_img)
        ext_pred = ext_logits.argmax(1).item()
        ext_probs = torch.softmax(ext_logits, dim=1)
        ext_conf = ext_probs[0, d].item()  # P(correct class)

        if ext_pred == d:
            ext_gen_correct += 1
        ext_gen_conf_sum += ext_conf

        gen_results[d] = {
            'image': gen_img[0].cpu(),
            'pred': ext_pred,
            'conf': ext_conf,
            'correct': ext_pred == d,
        }

ext_gen_acc = ext_gen_correct / 10
ext_gen_conf = ext_gen_conf_sum / 10

# æ‰“å°è¯„ä¼°ç»“æœ
print(f"{'='*60}")
print(f"  ç»¼åˆè¯„ä¼°ç»“æœ")
print(f"{'='*60}")
print(f"  Image->Text åˆ†ç±»å‡†ç¡®ç‡:       {test_acc*100:.2f}%")
print(f"  Image é‡å»º MSE:               {recon_mse:.4f}")
print(f"  Text->Image å¤–éƒ¨åˆ†ç±»å™¨å‡†ç¡®ç‡: {ext_gen_acc*100:.0f}%  (10ä¸ªæ•°å­—ä¸­{ext_gen_correct}ä¸ªè¢«æ­£ç¡®è¯†åˆ«)")
print(f"  Text->Image å¤–éƒ¨åˆ†ç±»å™¨ç½®ä¿¡åº¦: {ext_gen_conf*100:.1f}% (å¯¹æ­£ç¡®ç±»åˆ«çš„å¹³å‡æ¦‚ç‡)")
print(f"{'='*60}")
print(f"\n  é€æ•°å­—ç”Ÿæˆè´¨é‡:")
for d in range(10):
    r = gen_results[d]
    status = "âœ“" if r['correct'] else "âœ—"
    print(f"    æ•°å­— {d}: é¢„æµ‹={r['pred']} {status}  ç½®ä¿¡åº¦={r['conf']*100:.1f}%")


# =====================================================================
# 8) å¯è§†åŒ–
# =====================================================================
def to_img(tensor):
    """[-1, 1] -> [0, 1] çš„ç°åº¦å›¾"""
    return tensor.detach().cpu().squeeze() * 0.5 + 0.5

# --- (a) Image é‡å»ºå¯¹æ¯” ---
fig, axes = plt.subplots(2, 10, figsize=(12, 2.5))
for i in range(10):
    axes[0, i].imshow(to_img(sample_images[i]), cmap='gray'); axes[0, i].axis('off')
    axes[1, i].imshow(to_img(recon_images[i]),  cmap='gray'); axes[1, i].axis('off')
axes[0, 0].text(-0.15, 0.5, 'Original', transform=axes[0, 0].transAxes,
                fontsize=9, va='center', ha='right', fontweight='bold')
axes[1, 0].text(-0.15, 0.5, f'Recon\n(MSE={recon_mse:.4f})', transform=axes[1, 0].transAxes,
                fontsize=7, va='center', ha='right')
fig.suptitle('Image Reconstruction: Image -> Encoder -> Decoder -> Image', fontsize=11)
plt.tight_layout(rect=[0.08, 0, 1, 0.93])
save_plot("phase3_reconstruction.png")

# --- (b) Text -> Image ç”Ÿæˆ (0-9) ä¸çœŸå®æ ·æœ¬å¯¹æ¯” ---
real_examples = {}
for j in range(len(testset)):
    l = testset[j][1]
    if l not in real_examples:
        real_examples[l] = testset[j][0]
    if len(real_examples) == 10:
        break

fig, axes = plt.subplots(2, 10, figsize=(12, 2.5))
for d in range(10):
    r = gen_results[d]
    axes[0, d].imshow(to_img(r['image']), cmap='gray'); axes[0, d].axis('off')
    color = 'green' if r['correct'] else 'red'
    axes[0, d].set_title(f"{d} ({r['conf']*100:.0f}%)", fontsize=8, color=color)
    axes[1, d].imshow(to_img(real_examples[d]), cmap='gray'); axes[1, d].axis('off')
axes[0, 0].text(-0.15, 0.5, 'Generated', transform=axes[0, 0].transAxes,
                fontsize=9, va='center', ha='right', fontweight='bold')
axes[1, 0].text(-0.15, 0.5, 'Real', transform=axes[1, 0].transAxes,
                fontsize=9, va='center', ha='right', fontweight='bold')
fig.suptitle(f'Text -> Image Decoding  |  ExtAcc={ext_gen_acc*100:.0f}%  ExtConf={ext_gen_conf*100:.1f}%',
             fontsize=11)
plt.tight_layout(rect=[0.08, 0, 1, 0.93])
save_plot("phase3_text_to_image.png")

print("\nâœ… å®Œæˆï¼è¿™æ˜¯ç»è¿‡ä¼˜åŒ–çš„è·¨æ¨¡æ€è¯­ä¹‰å¯¹é½ + è§£ç å™¨ç”Ÿæˆæ–¹æ¡ˆã€‚")