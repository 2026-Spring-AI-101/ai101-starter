# -*- coding: utf-8 -*-
"""
MNIST è·¨æ¨¡æ€è¯­ä¹‰å¯¹é½ï¼ˆå…±äº«å‘é‡ç©ºé—´ï¼‰ç¤ºä¾‹
------------------------------------------------
æŠŠä½ åŸæ¥çš„ MNIST åˆ†ç±»ï¼ˆimage -> digitï¼‰å‡çº§ä¸ºï¼š
1) Image -> Text: çœ‹å›¾æ‰¾â€œæ–‡æœ¬æ ‡ç­¾â€ï¼ˆ0-9ï¼‰
2) Text -> Image: è¾“å…¥â€œæ•°å­—æ–‡æœ¬â€æ£€ç´¢æœ€åŒ¹é…çš„å›¾ç‰‡ï¼ˆTop-Kï¼‰

æ ¸å¿ƒæ€æƒ³ï¼š
- å›¾åƒç¼–ç å™¨ f_img è¾“å‡º D ç»´ embedding
- æ–‡æœ¬ç¼–ç å™¨ f_txtï¼ˆè¿™é‡Œç”¨æœ€ç®€å•çš„ 10 ç±» embedding lookupï¼‰è¾“å‡º D ç»´ embedding
- ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå‘é‡ç‚¹ç§¯ï¼‰åšåŒ¹é…ï¼›ç”¨äº¤å‰ç†µè®­ç»ƒâ€œå¯¹é½â€

è¿è¡Œæ–¹å¼ï¼š
    python mnist_shared_embedding.py

ç¬¬ä¸€æ¬¡è¿è¡Œä¼šè®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹ï¼›åç»­è¿è¡Œä¼šè‡ªåŠ¨åŠ è½½ã€‚
"""

import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import matplotlib
if os.environ.get('DISPLAY', '') == '':
    matplotlib.use('Agg')
import matplotlib.pyplot as plt

# -----------------------------
# Codespaces helper: save plots
# -----------------------------
# Codespaces is usually headless, so figures won't pop up. We save them to ./outputs/.
def save_plot(filename: str, dpi: int = 150):
    os.makedirs("outputs", exist_ok=True)
    out_path = os.path.join("outputs", filename)
    plt.savefig(out_path, dpi=dpi, bbox_inches="tight")
    print(f"[Plot saved] {out_path}")
    plt.close()

# -----------------------------
# 1) æ•°æ®åŠ è½½
# -----------------------------
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # æŠŠåƒç´ çº¦æŸåˆ° [-1, 1]ï¼Œä¸åŸè„šæœ¬ä¿æŒä¸€è‡´
])

trainset = torchvision.datasets.MNIST(root="/workspaces/ai101-starter/data_local/data", train=True, download=True, transform=transform)
testset  = torchvision.datasets.MNIST(root="/workspaces/ai101-starter/data_local/data", train=False, download=True, transform=transform)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)
testloader  = torch.utils.data.DataLoader(testset,  batch_size=256, shuffle=False)


# -----------------------------
# 2) æ¨¡å‹ï¼šå…±äº«å‘é‡ç©ºé—´ï¼ˆåŒç¼–ç å™¨ï¼‰
# -----------------------------
class ImageEncoder(nn.Module):
    """æŠŠå›¾ç‰‡ç¼–ç æˆ D ç»´å‘é‡"""
    def __init__(self, emb_dim: int = 32):
        super().__init__()
        self.fc1 = nn.Linear(28 * 28, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, emb_dim)

    def forward(self, x):
        x = x.view(-1, 28 * 28)
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)  # (B, D)
        return x


class SharedEmbeddingAligner(nn.Module):
    """
    - å›¾åƒç¼–ç å™¨ï¼šImage -> embedding (B, D)
    - æ–‡æœ¬ç¼–ç å™¨ï¼šdigit id (0-9) -> embedding (10, D)
    - ç›¸ä¼¼åº¦ï¼šcosine / dot product
    """
    def __init__(self, emb_dim: int = 32, num_text_tokens: int = 10, temperature: float = 0.07):
        super().__init__()
        self.image_encoder = ImageEncoder(emb_dim=emb_dim)
        self.text_embed = nn.Embedding(num_text_tokens, emb_dim)  # â€œæ–‡æœ¬â€è¿™é‡Œç®€åŒ–æˆ 10 ä¸ª token
        self.temperature = temperature

    @staticmethod
    def l2_normalize(x: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
        return x / (x.norm(dim=-1, keepdim=True) + eps)

    def encode_image(self, images: torch.Tensor) -> torch.Tensor:
        z = self.image_encoder(images)
        return self.l2_normalize(z)

    def encode_text_all(self) -> torch.Tensor:
        # ç›´æ¥å– embedding table çš„æ‰€æœ‰ tokenï¼ˆ0-9ï¼‰
        z = self.text_embed.weight  # (10, D)
        return self.l2_normalize(z)

    def logits_image_to_text(self, images: torch.Tensor) -> torch.Tensor:
        """
        è¿”å› (B, 10) çš„ç›¸ä¼¼åº¦ logitsï¼šæ¯å¼ å›¾å¯¹ 10 ä¸ªæ–‡æœ¬ token çš„åŒ¹é…ç¨‹åº¦
        """
        img_z = self.encode_image(images)          # (B, D)
        txt_z = self.encode_text_all()             # (10, D)
        logits = (img_z @ txt_z.t()) / self.temperature
        return logits  # (B, 10)


# -----------------------------
# 3) è®­ç»ƒ / è½½å…¥
# -----------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SharedEmbeddingAligner(emb_dim=32, temperature=0.07).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

model_path = "mnist_shared_embedding.pth"

def evaluate_accuracy(model: SharedEmbeddingAligner, dataloader) -> float:
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            logits = model.logits_image_to_text(images)
            pred = torch.argmax(logits, dim=1)
            correct += (pred == labels).sum().item()
            total += labels.numel()
    return correct / max(total, 1)


if os.path.exists(model_path):
    print("ğŸ”„ æ£€æµ‹åˆ°å·²æœ‰æ¨¡å‹ï¼Œæ­£åœ¨åŠ è½½...")
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
else:
    print("â³ æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹ï¼Œå¼€å§‹è®­ç»ƒï¼ˆå…±äº«å‘é‡ç©ºé—´å¯¹é½ï¼‰...")

    epochs = 5
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0

        for images, labels in trainloader:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            logits = model.logits_image_to_text(images)          # (B, 10)
            loss = criterion(logits, labels)                     # è®©æ­£ç¡®æ•°å­—çš„æ–‡æœ¬å‘é‡ä¸å›¾åƒå‘é‡æ›´æ¥è¿‘
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        train_acc = evaluate_accuracy(model, trainloader)
        test_acc = evaluate_accuracy(model, testloader)
        print(f"Epoch {epoch+1}/{epochs} | Loss: {running_loss/len(trainloader):.4f} | "
              f"Train Acc: {train_acc*100:.2f}% | Test Acc: {test_acc*100:.2f}%")

    torch.save(model.state_dict(), model_path)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³ {model_path}")


# -----------------------------
# 4) å¯è§†åŒ–ï¼šImage -> Textï¼ˆåƒåˆ†ç±»ä¸€æ ·é¢„æµ‹ï¼‰
# -----------------------------
def imshow(img_tensor_1x28x28):
    # åå½’ä¸€åŒ–åˆ° [0, 1]
    img = img_tensor_1x28x28 * 0.5 + 0.5
    npimg = img.cpu().numpy()
    plt.imshow(npimg.squeeze(0), cmap="gray")
    plt.axis("off")

model.eval()

# å–æµ‹è¯•é›†å‰ 10 å¼ å±•ç¤º
sample_images = torch.stack([testset[i][0] for i in range(10)], dim=0).to(device)  # (10,1,28,28)
with torch.no_grad():
    logits = model.logits_image_to_text(sample_images)
    predicted = torch.argmax(logits, dim=1).cpu().numpy()

plt.figure(figsize=(10, 4))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    imshow(sample_images[i])
    plt.title(f"é¢„æµ‹: {predicted[i]}")
plt.suptitle("Image -> Textï¼ˆå…±äº«å‘é‡ç©ºé—´ï¼šçœ‹å›¾é¢„æµ‹æ•°å­—ï¼‰", fontsize=12)
plt.tight_layout()
save_plot("phase2_image_to_text.png")


# -----------------------------
# 5) å¯è§†åŒ–ï¼šText -> Image æ£€ç´¢ï¼ˆè¾“å…¥æ•°å­—ï¼Œè¿”å› Top-K æœ€åƒçš„å›¾ç‰‡ï¼‰
# -----------------------------
def build_test_image_embeddings(model: SharedEmbeddingAligner, dataloader):
    model.eval()
    all_embs = []
    with torch.no_grad():
        for images, _ in dataloader:
            images = images.to(device)
            emb = model.encode_image(images)  # (B, D)
            all_embs.append(emb.cpu())
    return torch.cat(all_embs, dim=0)  # (N, D)

print("ğŸ“¦ æ­£åœ¨ä¸ºæµ‹è¯•é›†æ„å»ºå›¾åƒ embeddingï¼ˆç”¨äºæ£€ç´¢ï¼‰...")
test_image_embs = build_test_image_embeddings(model, testloader)  # (10000, D)

# é€‰æ‹©ä¸€ä¸ªæŸ¥è¯¢æ•°å­—ï¼ˆä½ ä¹Ÿå¯ä»¥æ”¹æˆ input() äº¤äº’ï¼‰
query_digit = 7
with torch.no_grad():
    # å–å¯¹åº”æ–‡æœ¬ embeddingï¼ˆè¿™é‡Œæ–‡æœ¬å°±æ˜¯ digit idï¼‰
    txt_emb_all = model.encode_text_all().cpu()  # (10, D)
    query_emb = txt_emb_all[query_digit:query_digit+1]  # (1, D)

# ç›¸ä¼¼åº¦ï¼šæ¯å¼ å›¾ä¸ query_emb çš„ç‚¹ç§¯ï¼ˆç­‰ä»·ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå› ä¸ºéƒ½å½’ä¸€åŒ–äº†ï¼‰
sims = (test_image_embs @ query_emb.t()).squeeze(1)  # (N,)
topk = 10
topk_vals, topk_idx = torch.topk(sims, k=topk)

# å–å‡º Top-K å›¾ç‰‡å±•ç¤º
retrieved_imgs = [testset[i][0] for i in topk_idx.tolist()]
retrieved_labels = [testset[i][1] for i in topk_idx.tolist()]

plt.figure(figsize=(10, 3))
for i in range(topk):
    plt.subplot(2, 5, i + 1)
    imshow(retrieved_imgs[i])
    plt.title(f"lbl:{retrieved_labels[i]}")
plt.suptitle(f'Text -> Image æ£€ç´¢ï¼šè¾“å…¥ "{query_digit}" è¿”å› Top-{topk}', fontsize=12)
plt.tight_layout()
save_plot(f"phase2_text_to_image_query{query_digit}_top{topk}.png")

print("âœ… å®Œæˆï¼šä½ å·²ç»å¾—åˆ°ä¸€ä¸ªæœ€å°å¯è¿è¡Œçš„è·¨æ¨¡æ€è¯­ä¹‰å¯¹é½ï¼ˆå…±äº«å‘é‡ç©ºé—´ï¼‰å®éªŒã€‚")